<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Chris Forster: Are Large Language Models Operationalizations of Saussurean Structure?
</title>
    <meta charset="UTF-8">

    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Metadata for Zotero -->
    <meta name="citation_author" content="Chris Forster">
    <meta name="citation_title" content="Are Large Language Models Operationalizations of Saussurean Structure?">
    <meta name="citation_date" content="2022-07-18">

    <link rel="stylesheet" type="text/css" href="/css/pygments.css" />
    <link rel="stylesheet" type="text/css" href="/css/style.css" />
    <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="cforster.com Atom feed" />
  </head>
  <body>

    <article class="post">
      <div class="frontmatter">
	<h1 class="title">Are Large Language Models Operationalizations of Saussurean Structure?</h1>
	<p><span class="author">Chris Forster</span> <br/> 
	  <time class="timestamp" datetime="2022-07-18" >July 18, 2022</time>
<!--	  <span class="length">4300 Words</span></p> -->
      </div>

      <div class="post-content">
	<p>The title’s a bit ugly, but it captures the question I’m trying to ask: can various computational representations of language/meaning be usefully understood as “operationalizations” of what Ferdinand de Saussure describes as structure or <em>langue</em> in the <em>Course in General Linguistics</em> (1916). Given Saussure’s influence in literary theory and the humanities more broadly,<span class="marginnote">I leave aside entirely Saussure’s impact in <em>linguistics</em>, though my sense is that Saussure was more put aside than superseded in linguistics. Insofar as computational approaches to language over the last twenty years represent something like a turn away from the dominance of Chomskyan grammar in the second half of the twentieth century, and towards something grounded in statistics, Saussure does seem a fitting figure.</span> I think this would be a useful way of enabling thinking about computational representations of meaning, from word embeddings (about which I’ll say more) to “large language models” and related technologies that have been kind of amazing to watch develop over the last year (I’m thinking of GPT-3, of DALL-E… all those kind of jaw dropping pictures circulating on twitter over the past few months).</p>
<p>I’m writing to see if this observation 1) is true, and 2) is useful. (I’m more uncertain of the latter than the former.) It seems to me that Saussure’s account of language and meaning is a useful analogue to computational models of meaning because it describes meaning as <strong>essentially differential</strong>. (Or, in a Derridean vein, <em>différantial</em>? … I want LLMs to bring theory back! Which is what Ted Underwood predicted… <a href="https://tedunderwood.com/2013/08/04/interesting-times-for-literary-theory/">nearly a <em>decade</em> ago</a>!) For instance, in trying to explain models like DALL-E to someone the other day, I was asked: “So, does it [DALL-E] invent something new, or just recombine things from the internet?”<span class="marginnote">Underwood captures a version of this in a <a href="https://twitter.com/Ted_Underwood/status/1512813693049917441">mini-thread invoking Coleridge’s distinction between fancy and imagination from the <em>Biographia Literaria</em></a>.</span> This question captures the challenge of thinking about, and thinking through, these models. I don’t think such models really do either; they don’t spontaneously create new images <em>ex nihilo</em> nor do they simply remix the already existing. But of course, this binary (creation/recombination) seems an impoverished way of thinking about how a human generates a text. The rather long and extensive, and now decades old, tradition of thinking about elements of culture through Saussure offers a valuable way out of this impasse. This very question was equally at stake in such (post)structuralist slogans as Roland Barthes’s insistence that “it is language which speaks, not the author” <span class="citation" data-cites="barthes_death_1977">(Barthes 143)</span> (a connection Underwood has also made <a href="https://twitter.com/Ted_Underwood/status/1536681676125913088">[cite]</a>). At the very least, thinking about these models as Saussurean structures (rather than as, say, <em>intelligences</em>) can forestall some of the <a href="https://www.washingtonpost.com/technology/2022/06/11/google-ai-lamda-blake-lemoine/">worst ways</a> of trying to understand what these models output.</p>
<p>The structuralist overtones of much computationally-driven cultural work has often come in a Lévi-Straussian flavor, mixed with a desire for a more empirically grounded work. But that is, I think, only one half (and, I think, the less influential half) of how structuralism, and the “theory moment,” shaped literary theory (to stick to the area with which I’m most familiar). Returning to Saussure gives an opportunity to enrich and complicate the structuralist/computational connection. I want to suggest that these models offer an opportunity to highlight the essentially <em>quantitative</em> element that underlies Saussure and, indeed, remains implicit in much post-Saussurean and poststructuralist thinking. This quantitative element is essential, but largely latent, in Saussure’s description of the arbitrary character of the sign.</p>
<p>I think the broad strokes (and perhaps even the narrow strokes) of the connections between computational models of language (including “large language models”) and structuralism seem like they’ve been in the ether for a while. For instance:</p>
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">
Sure, they didn't want to be proved right <em>like this.</em> But that's always true of prophecy. Poets and literary critics are still morally required to take a victory lap for predicting that “it is language which speaks, not the author.” <a href="https://t.co/Exw4VGXGZs">pic.twitter.com/Exw4VGXGZs</a>
</p>
— Ted Underwood (<span class="citation" data-cites="Ted_Underwood">(<strong>Ted_Underwood?</strong>)</span>) <a href="https://twitter.com/Ted_Underwood/status/1536681676125913088?ref_src=twsrc%5Etfw">June 14, 2022</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">
“Lang.” does seem better for current AI than “intelligence.” But interesting how “lang.” here closer to Saussure's “langue” (abstract signifying system) than “parole” (actual utterances). Isn't lang used as “foundation” model bc it provides a useful starterpack signifying system?
</p>
— Ryan Heuser (<span class="citation" data-cites="quadrismegistus">(<strong>quadrismegistus?</strong>)</span>) <a href="https://twitter.com/quadrismegistus/status/1526110404111437826?ref_src=twsrc%5Etfw">May 16, 2022</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">
structuralism is back baby!! <a href="https://t.co/60gRTTgBQc">https://t.co/60gRTTgBQc</a>
</p>
— Robin Manley (<span class="citation" data-cites="_robinmanley">(<strong>_robinmanley?</strong>)</span>) <a href="https://twitter.com/_robinmanley/status/1518989544544038912?ref_src=twsrc%5Etfw">April 26, 2022</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>These tweets all to some extent express the idea that there is something about language models/neural networks/“artificial intelligence” that recalls structuralism. Bernard Dionysus Geohegan’s work on cybernetics and structuralism and his forthcoming book <a href="https://www.dukeupress.edu/code"><em>Code: From Information Theory to French Theory</em></a> promises to enrich the deeper history of this connection; but I want to focus on something far more modest.</p>
<p>Indeed, I started this post originally to talk about something much narrower—J. R. Firth, and the theorization of word embedding vectors.</p>
<h2 id="on-j.-r.-firth-as-theoretical-inspiration-for-word-embedding-vectors">On J. R. Firth as Theoretical Inspiration for Word Embedding Vectors</h2>
<p>Word embeddings, or word embedding vectors (among other methods), are a way of representing the meaning of a word with a series of numbers. The basic idea of word embeddings is to treat a word as a point in (multidimensional) “space.”<span class="marginnote">Listen, writing about these things means going out on a limb. If there is something grossly wrong this summary, let me know.</span> A word’s position in space reflects, or represents, its meaning. Most simply: words with similar meanings should be grouped together in the same space. <em>Schnauzers</em>, <em>bulldogs</em>, and <em>poodles</em> might all be constellated around <em>dog</em>. A lovely idea, right? But how to do it? (<a href="https://ryanheuser.org/word-vectors/">Ryan Heuser’s posts on word vectors</a> were an invaluable resource when I first read them.)</p>
<p>Key techniques for generating these vectors <em>embed</em> a word in this “semantic” space by looking at the terms that surround it. If you had a massive dataset and looked at the words that appear “near” (say, within 2 or 3 words) the terms for dog breeds I just mentioned, you would find terms for the sort of things dogs do—wag tails, fetch bones, mark territory, prick up their ears, get dewormed, and so on.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Now we can use those terms to generate the coordinates for each of our dog-related terms. Since words with similar meanings have similar collocates, this should get us our space, right? Carrying this intuition further, we would assume that <em>cat</em> as a term will not be closer in this space to <em>bulldog</em> or <em>schaunzer</em> than both of those terms will be to <em>dog</em>. But cat will be closer to <em>dog</em> (and any other words in that “neighborhood”) than it is to say “Constitution” or “fascism” (to pick two words chosen entirely without regard to contemporary events here in the summer of 2022).</p>
<p>From collocation of words to locations of vectors—that’s the idea.<span class="marginnote">All fine for the English professor to say; but transforming collocation counts into a meaningful representation is going to involve some linear algebra.</span> When people were first oohing and aahing over these things (back in 2015), one could use some relatively straightforward vector algebra to do things like querying for the resulting term if one started with the position of <em>king</em>, subtracted the embedding vector representing <em>man</em>, and added in the vector for <em>woman</em>. <code>king - man + queen</code> produced, <code>queen</code>, which is pretty impressive… or, it was in 2015. (Compared to the outputs of DALL-E and GPT-3, this all seems wonderfully quaint.)<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>In reading the literature about word embedding vectors, the most frequently cited theoretical inspiration for this approach to meaning is the British linguistic J. R. Firth. He is quoted with great frequency for asserting that “You shall know a word by the company it keeps!” in “A Synopsis of Linguistic Theory” <span class="citation" data-cites="firth_synopsis_1968">(179)</span>.<span class="marginnote">This essay can be a bit tricky to track down. If you’d like a PDF, email/DM me.</span> Firth insists that meaning be grounded in a “context of situation,” a concept that sounds vaguely related to the “company [a word] keeps,” but the former is a much broader idea. And for all the citations Firth gets as inspiration for semantic word vectors, they seem rather far afield of Firth’s central preoccupations in “A Synopsis.” Indeed, reading beyond this passage, one finds very little to help you reason about collocations quantitatively.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>Firth, in short, provides only the thinnest veneer of theoretical justification for what word embeddings are, and how they work. An essay by Mikael Brunila and Jack LaViolette (<a href="https://arxiv.org/abs/2205.07750">a May 2022 preprint</a>) summarizes this situation well, discussing J. R. Firth alongside Zelig Harris as the theoretical inspirations for word embeddings. As they note, in the word embedding literature “despite an explosion of citations… this interest [in Firth] has not been very engaged.” Rather, Firth has been invoked in order to “lend theoretical authority to a field that struggles to lift its gaze from the latest state-of-the-art numbers” <span class="citation" data-cites="brunila_what_2022">(Brunila and LaViolette)</span>. If you come to Firth from discussions of word embeddings (as I did), you may feel rather puzzled (as I did!), looking (in vain!) for collocation calculations among talk of “context of situation” and the occasional citation of Alfred North Whitehead (!).</p>
<p>The reason Firth’s thinking, or at least Firth’s slogan that meaning is related to the “company [a word] keeps,” is so often cited is because the technique used by <a href="https://code.google.com/archive/p/word2vec/">word2vec</a>, one of the most influential word embedding models, uses a small window of collocates to generate its vectors. But this window is more a detail of implementation, rather than an essential element in how one might imagine word embedding models (indeed, other word embeddings—including <a href="https://nlp.stanford.edu/projects/glove/">GloVe</a>, unless I’m mistaken, don’t use this window). If the core insight of word vectors is to imagine <em>meaning</em> as a point in large dimensional space, then Firth may not be the most useful theoretical inspiration. The space these models create is generated by representing a term not simply in relation to its collocates, but also in relation to all the terms in the vocabulary. The collocates embed a term in a space that is generated (originally) by all the terms in the model’s vocabulary. It is the sum total of all the terms that are used, and then how they used in relation to one another, that really allows meaning to be represented and mapped in this spatial way.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>So, we’re not just talking about “the company [a word] keeps,” we’re talking about a method for generating a “space.” Yet even representing meaning as points in <em>space</em> is, to some degree a technical detail (or, perhaps, a metaphor). Trying to imagine what a 300-dimensional word embedding model “looks like” is, of course, impossible.<span class="marginnote">I recall a rather funny comment somewhere: asked how to imagine, say, a twelve-dimensional space, a mathematician responded: “Imagine a three dimensional space. Now concentrate while saying 12 loudly.”</span> While the algorithms and measures of distance used in such models have geometrical interpretations that make them easier to understand, what we’re really talking about is not “space” as we (non-mathematicians) visualize or imagine it. Or, we might say, it is very easy to imagine what a 300-dimensional word embedding looks like: it looks like a spreadsheet with 300 columns, and rows for each term in the vocabulary. What these models are quantifying and representing is not space, but the <em>differences</em> among the terms.</p>
<p>So this is my point: if these models are best understood as representing language (or meaning) as a system of differences, then it is Saussure, far more than Firth, who provides something like a theoretical description of this model. A set of word embedding vectors, or indeed most language models as I (in my inexpert way understand them) look very much like what Saussure calls “a system of pure values” <span class="citation" data-cites="saussure_course_1986">(155)</span>.</p>
<h2 id="saussure">Saussure</h2>
<p>So, let’s talk Saussure for a moment. My goal here is simply to highlight the (I think) often ignored <em>quantitative</em> element that is implicit in Saussure’s model of meaning, in order to ask whether contemporary, computational language models might be described as operationalized instances of a Saussurean model of meaning.</p>
<p>Central to Saussure’s description of language is a rejection of any attempt to treat it as a list-like aggregation of names paired with concepts/ideas. One instead had to treat the system as a whole: “A language is a system in which all the elements fit together, and in which the value of any one element depends on the simultaneous coexistence of all the others” <span class="citation" data-cites="saussure_course_1986">(Saussure et al. 159)</span>. “Value” here is a slightly slippery term. Saussure calls the exchange of a word for an idea <em>meaning</em>, but the relationship a word has to the rest of the words in a language its <em>value</em>. A value is determined not in the relationship between a word and the object it names, but in relationship to the other words within a language/system.</p>
<p>Saussure offers an illuminating example of how <em>value</em> differs from <em>meaning</em>:</p>
<blockquote>
<p>The French word <em>mouton</em> may have the same meaning as the English word <em>sheep</em>; but it does not have the same value. There are various reasons for this, but in particular the fact that the English word for the meat of this animal, as prepared and served for a meal, is not <em>sheep</em> but <em>mutton</em>. The difference in value between <em>sheep</em> and <em>mouton</em> hinges on the fact that in English there is also another word <em>mutton</em> for the meat, whereas <em>mouton</em> in French covers both.</p>
<p>In a given a language, all the words which express neighboring ideas help to define one another’s meaning. Each set of synonyms like <em>redouter</em> (‘to dread’), <em>craindre</em> (‘to fear’), <em>avoir peur</em> (‘to be afraid’) has its particular value only because they strand in contrast with one another. If <em>redouter</em> (‘to dread’) did not exist, its content would be shared out among its competitors… So the value of any given word is determined by what other words there are in that particular area of the vocabulary. <span class="citation" data-cites="saussure_course_1986">(160)</span></p>
</blockquote>
<p>Values don’t pre-exist the system, waiting to be labeled; they are, as we say, produced by the language. The key I want to highlight, however, is how strongly this description resonates with word vectors (and, I think, with computational models of language/meaning more broadly). Saussure’s language here—of a “neighborhood” of terms; of a “particular area of the vocabulary”—anticipates models of meaning as vector space. Saussure’s description here is, I think, a better description/theorization of word-embedding vectors (and similar models) than Firth’s “company it keeps” slogan.</p>
<p>After a discussion of differences in inflection systems and verb tenses across language (as another example, along the <em>mutton</em>/<em>mouton</em> distinction, of how values are produced by language), Saussure summarizes:</p>
<blockquote>
<p>In all these cases what we find, instead of <em>ideas</em> given in advance, are <em>values</em> emanating from a linguistic system. If we say that these values correspond to certain concepts, it must be understood that the concepts in question are purely differential. That is to say they are concepts defined not positively, in terms of their content, but negatively by contrast with other items in the same system. What characterizes each most exactly is being whatever the others are not. <span class="citation" data-cites="saussure_course_1986">(162)</span>.</p>
</blockquote>
<p>This reflects what Saussure articulates as a “paradoxical principle”: the doubleness of all values. <strong>One</strong>: a value can be located within a system of like/comparable values; <strong>two</strong>, value can also be exchanged that for something dissimilar. Saussure’s example is money, which can be compared to other monetary values quantitatively, or exchanged for some object/commodity. In language a word can be compared to other like values (i.e. to other words)—but also “exchanged” for something dissimilar (a concept or an idea). When Saussure articulates this idea, he simply asserted that comparing words to other words is somehow akin to the quantitative comparison of one monetary value to another. Vector-space representations of language, I think we have a more direct model of what this could actually look like.</p>
<figure>
<img src="/images/saussure_signs.png" alt="Saussure’s image of a language as a system." /><figcaption aria-hidden="true">Saussure’s image of a language as a system.</figcaption>
</figure>
<p>This crude image of signs relating to one another in a total system seems to show each term relating only to its neighbors—in a sort of one dimensional line. But the idea, I take it, is that each term is a single location within a much larger space, and is defined only by its difference from all the other terms. Some terms, of course, may be nearer and further—but the space is defined by all the terms, by the <em>langue</em>.</p>
<p>This understanding of meaning as <em>differential</em>, as produced by a value’s (quasi-quantitative) difference from other (quasi-quantiative) values, is at the heart of—indeed, I’m suggesting it <em>is the heart</em> of—the Saussurean model of meaning. Saussure, of course, doesn’t quantify his values, and doesn’t seem to imagine the uses or benefits of doing so. But the idea of meaning as essentially differential is, I think, a fundamentally quantitative (even if only implicitly so) concept.</p>
<p>In the context of intro lit theory classes (of the sort that I teach with some regularity), I think the stress is sometimes placed on Saussure’s insistence on the arbitrary character of the sign. From this arbitrariness stems the antifoundationalism that is a hallmark of poststructuralism. But, for my money (and, probably, Derrida’s), it is the claim that languagef is a system of differences without any positive values that is the genuinely interesting element of the <em>Course in General Linguistics</em>. After all, the observation that the signifier is “arbitrary” is neither novel to Saussure, nor particularly shocking. Saussure’s insight is that if, as seems incontestably obvious, there is no necessary relationship between what he calls signifier and signified (a fact well evidenced by the existence of different languages)<span class="marginnote">Saussure does spend some a brief passage rejecting any attempt to ground meaning in onomatopoeia <span class="citation" data-cites="saussure_course_1986">(101–02)</span>.</span>, then arbitrary <em>means</em> differential: “the terms <em>arbitrary</em> and <em>differential</em> designate two correlative principles” <span class="citation" data-cites="saussure_course_1986">(Saussure et al. 163)</span>. Large language models represent an operationalization of this idea, they lend calculations to what was always already (ahem) a quantitative concept.</p>
<h2 id="so-what">So what?</h2>
<p>One reason I’ve insisted at such length on this comparison is because it seems to offer a bridge between two very different ways of thinking. One observation I’d like to offer is that structuralist, and structuralist-influenced thinking, and so a wide range of the sort things that folks encounter in humanities classes like “literary theory,” have a quantitative dimension that tends to be ignored. Judith Butler’s <em>Gender Trouble</em> draws a distinction between its own position and structuralism. Butler writes:</p>
<blockquote>
<p>The <em>totality</em> and <em>closure</em> of language is both presumed and contested within structuralism. Although Saussure understands the relationship of signifier and signified to be arbitrary, he places the arbitrary relation within a necessarily complete linguistic system. All linguistic terms presuppose a linguistic totality of structures, the entirety of which is presupposed and implicitly recalled for any one term to bear meaning. This quasi-Liebnizian view, in which language figures as a systemic totality, effectively suppresses the moment of difference between signifier and signified, relating the unifying that moment of arbitrariness within a totalizing field. The poststructuralist break with Saussure… refutes the claim of totality and universality and the presumption of binary structural oppositions that implicitly operate to quell the insistent ambiguity and openness of linguistic and cultural signification. <span class="citation" data-cites="butler_gender_1990">(40)</span></p>
</blockquote>
<p>Yet <em>Gender Trouble</em> shares with Saussure a commitment to the arbitrariness of the sign. And the “insistent ambiguity and openness of linguistic cultural signification” that is so essential to Butler’s larger description of gender’s <em>performativity</em> (and the potential politics that this insistent need for re-iteration demands), is not less quantitative/differential than Saussure’s description of <em>langue</em>. Such signifying phenomena—defined by iterability, by arbitrariness, by difference (or <em>différance</em>)—imply a differential “matrix” (a term used by poststructuralists and machine learning enthusiasts alike), and so are in some sense quantitative. I’m <em>emphatically</em> not interested in trying to actually operationalize Butler’s account of gender—to attach numbers to particular gendered signifiers, say—but it seems that Butler’s notion of gender, in desubstantializing gender, makes it instead an essentially stochastic concept. Perhaps some find that unsurprising—but not me.</p>
<p>I also think this connection can help understand, or at least redescribe, some of the most common problems with large language models. We might, for instance, note the a difference between Saussure’s description of meaning generation (a process that, for Saussure, includes language but many other systems besides) and language models that rests on the <em>langue/parole</em> distinction. Saussure infamously insists on a rigid distinction between the abstract system that a language constitutes (<em>langue</em>) and any particular use/event of that system (speech or <em>parole</em>). The structuralist, in Saussure’s description, studies <em>langue</em>, a synchronic system imagined outside of its history, rather than any particular use or <em>parole</em>. While a vector-space model of meaning (like word embeddings), I’ve suggested, operates like <em>langue</em>, it is built entirely out of <em>parole</em>. It scoops up massive amounts of text in order to transform what Saussure calls <em>values</em> into numeric values. Can we understand issues of embedded bias <a href="https://nyupress.org/9781479837243/algorithms-of-oppression/">described by Safiya Noble</a>, for instance, as a consequence of treating <em>parole</em> as <em>langue</em>? Does such a redescription enable our thinking in any productive way?</p>
<p>Finally, Saussure reminds us of the inherently social location of the “intelligence” these models are able to achieve. If these models present us with uncanny images of apparent intelligence, it is useful to recall that the very “structure” that these models manage to boil down and concentrate from so many instances of <em>parole</em> is an intelligence that we have collectively put there. David Weinberger famously insisted, in what became a sort of slogan in the mid 2010s among a certain style of tech enthusiasm, that “the smartest person in the room is the room.” It’s a description of the power of networked culture. But, of course, a room is never “the smartest person” “in the room”; a room is neither smart nor a person. Weinberger’s eminently reasonable point—(crudely, that people working together have more intelligence than any individual, and that networks enrich/enable this)—is fair enough, and well taken! The danger is to localize and attribute this outcome of social organization to a particular technology (here, “the room”).<span class="marginnote">(See also Jerome McGann, <em>A New Republic of Letters</em>, pg. 43).</span> In discussions of language models, I think we see something similar—a misattribution of collective, cultural “knowledge”/“intelligence” (of the sort that is embedded in language use) to a mechanism/technology for extracting/re-presenting that intelligence. I’m inclined to see Saussure’s model of meaning as helping us to see that what looks like “intelligence” in these models is a system of relationships that inheres in the material they’re trained on. It is the position of the points that is the source of the word embeddings power, and that position reflects how the words are used. The embeddings, or other technologies, can reveal some surprising and shocking and impressive things; they are truly remarkable. But it is the system of differences—or the odd pieces of it that are carved off and thrown into the flow of GPU-powered tensors—which is the location of the “intelligence” such models achieve, far more than than the models themselves.</p>
<h2 class="unnumbered" id="works-cited">Works Cited</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-line-spacing="2" role="doc-bibliography">
<div id="ref-barthes_death_1977" class="csl-entry" role="doc-biblioentry">
Barthes, Roland. <span>“The <span>Death</span> of the <span>Author</span>.”</span> <em>Image, <span>Music</span>, <span>Text</span></em>, translated by Stephen Heath, <span>Hill and Wang</span>, 1977, pp. 142–48.
</div>
<div id="ref-brunila_what_2022" class="csl-entry" role="doc-biblioentry">
Brunila, Mikael, and Jack LaViolette. <em>What Company Do Words Keep? <span>Revisiting</span> the Distributional Semantics of <span>J</span>.<span>R</span>. <span>Firth</span> &amp; <span>Zellig Harris</span></em>. arXiv:2205.07750, <span>arXiv</span>, 16 May 2022, <a href="https://doi.org/10.48550/arXiv.2205.07750">https://doi.org/10.48550/arXiv.2205.07750</a>.
</div>
<div id="ref-butler_gender_1990" class="csl-entry" role="doc-biblioentry">
Butler, Judith. <em>Gender <span>Trouble</span>: <span>Feminism</span> and the <span>Subversion</span> of <span>Identity</span></em>. <span>Routledge</span>, 1990.
</div>
<div id="ref-firth_synopsis_1968" class="csl-entry" role="doc-biblioentry">
Firth, J. R. <span>“A Synopsis of Linguistic Theory, 1930-55.”</span> <em>Selected <span>Papers</span> of <span>J</span>. <span>R</span>. <span>Firth</span> 1952-59</em>, edited by F. R. Palmer, <span>Indiana University Press</span>, 1968, pp. 168–205.
</div>
<div id="ref-saussure_course_1986" class="csl-entry" role="doc-biblioentry">
Saussure, Ferdinand de, et al. <em>Course in general linguistics</em>. <span>Open Court</span>, 1986.
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>These sorts of intuitions—our sense of what terms are likely to be near the word “dog,” for example—crop up frequently in discussions of these methods. They offer useful “just so” stories that inspire these methods; yet it is not clear to me that these useful examples are true. We know that dogs do all those things I mentioned—but a quick look at the 296 times the term “dog” appears in the English texts in Andrew Piper’s <a href="https://txtlab.org/2016/01/txtlab450-a-data-set-of-multilingual-novels-for-teaching-and-research/">txtLab450</a> doesn’t show any of those things. In those 296 references, within a small window of nearby terms, a dog wags its tail only once; never does a dog fetch. Dogs twice dig up bones (though only metaphorical dogs, digging metaphorical bones). My point is simply that as we reason about how these sorts of methods work, we often rely upon some reasonable intuition. Yet those reasonable intuitions don’t always seem to be supported by the method, and may paper over how genuinely alien these methods are to our basic intuitions.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>And that example seems carefully chosen. Following <a href="https://douglasduhaime.com/posts/clustering-semantic-vectors.html">Douglas Duhaime’s helpful tips</a>, and this <a href="https://medium.com/analytics-vidhya/basics-of-using-pre-trained-glove-vectors-in-python-d38905f356db">medium post</a>, we can easily reproduce it. But my dog examples <em>do not</em> work. If we try something the words nearest the vector produced <code>bulldog - dog + cat</code> we get: <code>'cat', 'myoot', 'bohld', 'js04bb', 'kahrd', 'kd96', 'rw95', 'greg.wilcoxdailynews.com'</code>, so… yeah.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Anyone looking for the full context of the original quote. Here it is, including an interesting invocation of Wittgenstein:</p>
<blockquote>
<p>The <em>placing</em> of a <em>text</em> as a constituent in a context of situation contributes to the statement of meanign since situations are set up to recognize <em>use</em>. As Wittgenstein says, “the meaning of words lies in tehir use.” The day-to-day practice of palying language games recognizes customs and rules. It follows that a text in such established usage may contain sentences such as ‘Don’t be an ass!’ ‘You silly ass!’ ‘What an ass he is!’ In these examples the word <em>ass</em> is in familiar and habitual company, commonly collocated with <em>you silly—</em>, <em>he is a silly—</em>, <em>don’t be such an—</em>. You shall know a word by the company it keeps! One of the meanings of <em>ass</em> is its habitual collocation with such other words as those quoted. Though Wittgenstein was dealing with another problem, he also recognizes the plain face-value, the physiogonomy of words. ‘The sentence is composed of the words and that is enough.’ <span class="citation" data-cites="firth_synopsis_1968">(Firth 179)</span>.]</p>
</blockquote>
<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></li>
<li id="fn4" role="doc-endnote"><p>Key to word embeddings is reducing the dimensionality of this representation. This means that a set of texts which may feature a vocabulary in the hundreds of thousands (or greater!) can be represented with a “mere” few hundred dimensions (the <a href="https://nlp.stanford.edu/projects/glove/">pretrained GloVe embeddings</a> are offered in 50, 100, 200, and 300 dimension versions). Like the most sophisticated language models and image generators (like GPT-3, DALL-E), these embeddings are generated (“learned”) using neural networks to reduce their dimensionality. A thing I still don’t understand: if the complete model were computationally tractable, would it be superior to using this significantly smaller model? Or, in reducing the dimensionality, do these models help make the vectors more “meaningful” by “squishing” things closer together, and so making the meaning more apparent to the various ways in which the model can be queried?<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

      </div>
<!--      <div id="disqus_thread"></div>
      <script type="text/javascript"
	      src="http://disqus.com/forums/chrisforstersblog/embed.js">
      </script>
      <noscript>
	<a href="http://chrisforstersblog.disqus.com/?url=ref">View the discussion thread.</a>
      </noscript>
      <a href="http://disqus.com" class="dsq-brlink">
	blog comments powered by <span class="logo-disqus">Disqus</span>
      </a>
-->
    </article>
  </body>

</html>